---
layout: post
title: "Extended Kalman Filter SLAM"
date: "2019-09-20"
# slug: "example_content"
description: "This post is based on SLAM lecture"
category: 
  - SLAM
# tags will also be used as html meta keywords.
tags:
  - SLAM 
comments: true
use_math: true
---
> 본 게시글은 Cyrill Stachniss의 [Robot mapping(WS 2013/14)](http://ais.informatik.uni-freiburg.de/teaching/ws13/mapping/) 강의를 요약 및 정리한 글입니다.   
> 틀린 내용을 지적해주시면 감사드리겠습니다.

# EKF(Extended Kalman Filter) SLAM

- SLAM을 위한 방식으로는 크게 3가지 방식이 있다.
  - Kalman filter based, Particle filter based, Graph-based
- 이 장에서는 Extended Kalman Filter based SLAM에 대해 이야기한다.
  

## EKF Online SLAM

### Goal

- $p(x_t, m \| z_{1:t}, u_{1:t})$ 를 추정하는 것
- 즉, robot의 pose와 landmark의 위치를 추정하는 것

#### EKF SLAM Model

- Assumption : measurement가 어떤 landmark에 속하는지를 미리 알고 있다.(known corespondence)  
  -> 즉, data association 단계가 필요없다.

##### State space

- $x_t = (x,y,\theta, m_{1,x}, m_{1,y}, ... , m_{n,x}, m_{n,y})$
- $x,y,\theta$ : robot의 pose, 3차원이 되면 $x,y,z,\theta,\phi,\psi$
- $m_{n,x}, m_{n,y}$ : landmark의 위치, 3차원이 되면 $m_{n,z}$ 추가
- 3+2N dimension
- 만약, unknown correspondence라면 $m_{n,x}, m_{n,y},s_{n}$ 으로 $s_n$이 추가된다.
- 만약, 3차원 + unknown correspoendence 라면 6+3N dimension

##### state representation

- 2차원, known correspondence 환경에서는 state가 아래와 같이 3+2N dimensional gaussian으로 표현된다.

![](/images/posts/SLAM/2019-09-20-EKF-SLAM2/img1.png)

![](/images/posts/SLAM/2019-09-20-EKF-SLAM2/img2.png)



### EKF SLAM Filter Cycle

- EKF Algorithm
  1. $\bar{\mu_t}=g(u_t,\mu_{t-1})$
  2. $ \bar{\sum{} _t}=G_t\sum{}_{t-1}G_t^T + R_t $
  3. $K_t=\bar{\sum{}_t}H_t^T(H_t\bar{\sum{}_t}H_t^T+Q_t)^{-1}$
  4. $\mu_t=\bar{\mu_t}+K_t(z_t-h(\bar{\mu_t}))$
  5. $\sum{}_t=(I-K_tH_t)\bar{\sum{}_t}$

- EKF SLAM은 아래 다섯 가지 단계로 이루어진다.

  1. State prediction

  2. Measurement prediction

  3. Measurement

  4. Data association

  5. Update

     

##### 1. State prediction

![](/images/posts/SLAM/2019-09-20-EKF-SLAM2/img3.png)

- state prediction에서는 로봇이 motion을 취했을 때,  발생하는 motion noise를 고려하여 불확실성이 증가한다.
- 로봇 모션 자체의 불확실성 $\sum{}x_Rx_R$ 뿐만 아니라 로봇과 map 사이의 correlation($\sum{}x_Rm_j$도 존재한다.
- robot command는 오직 robot의 pose만 영향을 주기 때문에 초록색 부분만 update된다.
- 즉, $N^2$ 의 복잡도가 필요하다.



##### 2. Measurement prediction

![](/images/posts/SLAM/2019-09-20-EKF-SLAM2/img4.png)

- measurement prediction은 내가 예측한 현재 robot pose의 위치 $\mu_t$ 와 sensor system, $h(x_t)$ 를 기반으로 measurement 를 예측하는 것이다.  -> $h(\mu_t)$

  

##### 3. Measurement

![](/images/posts/SLAM/2019-09-20-EKF-SLAM2/img5.png)

- 실제로 측정한 센서 값 $z_t$ 를 구한다.



##### 4. Data Association and Difference Between h(x) and z

![](/images/posts/SLAM/2019-09-20-EKF-SLAM2/img6.png)

- __Data Association 부분은 추후에 공부 후 추가.__

- EKF Algorithm의 4번째, $\mu_t=\bar{\mu_t}+K_t(z_t-h(\bar{\mu_t}))$ 에서 Measurement와 Meas. prediction의 차이를 구하고 kalman gain을 곱하는 단계에 해당된다.

  

##### 5. Update

![](/images/posts/SLAM/2019-09-20-EKF-SLAM2/img7.png)

- 알고리즘의 4, 5번째 단계로 최종적으로 state를 update한다.
- 이때, robot pose 뿐만 아니라 landmark 까지 모두 update되므로 $O(N^3)$의 복잡도가 발생한다.



#### Concrete EKF SLAM Example

##### 1. Initialization

- Robot pose : 로봇의 시작 위치를 좌표의 기준으로 삼는다.  
  -> 즉, 항상 로봇의 시작 위치는 정확하게 알 수 있다.   
  ->  $\mu = (0,0,0), R=0$ 
- Landmark location : 랜드마크에 대해 전혀 알 수 없다.  
  -> $\mu =0, R= \infty$ 

![](/images/posts/SLAM/2019-09-20-EKF-SLAM2/img8.png)

##### 2. Prediction step

![](/images/posts/SLAM/2019-09-20-EKF-SLAM2/img9.png)

- 2에서 첫 3 column은 robot pose에 해당되고 나머지는 landmark에 해당된다.  
  -> 즉, prediciton은 robot의 pose에만 영향을 미친다.
- $R_t^x$는 현재 motion model의 covariance로 3x3 matrix이다.  
  -> 역시 robot의 pose에만 영향을 준다.

##### 3. Correction step

![](/images/posts/SLAM/2019-09-20-EKF-SLAM2/img10.png)

![](/images/posts/SLAM/2019-09-20-EKF-SLAM2/img11.png)

- $Q_t$ : 미리 정의된 노이즈
- i : landmark index,  j : measurement index  
  -> 여러 measurement가 한 landmark에 해당될 수 있다.
- $F_{x,j}$ : robot pose 와 j 번째 measurement가 탐지한 i번째 landmark를 update 한다.
- $H_t^i$ : Measurement Jacobian, 하나의 landmark에 해당된다.  
  2개의 행은 $r, \phi$ 에 해당되며 5개의 행은 $x,y,\theta,m_{i,x},m_{i,y}$에 해당된다.
- $H_t^i$를 (2N) X(3+2N) matrix로 만들면 for 문을 돌리지 않고 한번에 계산할 수 있다.  
  -> $F_{x,j}$와 비슷한 형태로, 굉장히 sparse한 matrix

- 18에서 $z_t^i - \bar{z_t}^i$ 을 계산할 때, $\theta - \bar{\theta}$ 에서 각도를 normailze 안하면 매우 큰 오차가 발생할 수 있다.



## Loop Closing

- loop closing이란?
  - 이미 map을 그렸던 지역을 다시 인식하는 것, 즉, 돌고 돌아 다시 원점으로 돌아오고 인식하는 것
  - 다시 원점을 왔을 때, 원점 근처의 불확실했던 landmark들의 covariance는 매우 감소한다.
  - 먼 지역의 landamarm들은 여전히 불확실함.
  - robot pose 역시도 불확실성이 매우 감소
  
  ![before loop closure](/images/posts/SLAM/2019-09-20-EKF-SLAM2/img12.png)
  
  - before loop closure
  
  ![After loop closure](/images/posts/SLAM/2019-09-20-EKF-SLAM2/img13.png)
  
  - After loop closure
  
- Data association의 어려움
  - 한바퀴 돌아오면 굉장히 불확실성이 높아진 상태
  - 대칭적인, 비슷한 지역을 원점으로 인식할 수도 있음
  - 만약 loop closing을 잘못하면 불확실성은 무한대로 증가, 발산할 수 있다.

- 동그라미 친 원점에서 loop closing에 성공하면 근처 landmark의 불확실성은 크게 줄어든다.
  
  - 굉장히 정확한 원점(출발점)과 강한 correlation이 있으므로



### EKF SLAM Correlation

![](/images/posts/SLAM/2019-09-20-EKF-SLAM2/img14.png)

- landmark 간에도 강한 correlation이 있는 것을 확인할 수 있다.
- 모든 landmark들은 fully correlated 되어있다.
- $r_{xi,xj}, r_{yi,yj}$ 의 correlation은 크지만 $r_{xi,yj}$의 correlation은 크지 않다.  
  -> 직관적으로 첫 번째 landmark가 +x 방향으로 조금 밀려서 관측됐다면 두 번째 landmark역시 +x방향으로 밀리겠지만 y방향으로의 영향은 독립적이다.



### EKF SLAM Uncertainties

![](/images/posts/SLAM/2019-09-20-EKF-SLAM2/img15)

- 새로 관측된 landmark의 불확실성은 무한대이다.
- 계속해서 관측된다면 landmark의 불확실성은 감소한다.
- 아무리 많이 관측하더라도 최초의 robot pose의 covariance보다 작아질 수 없다.  
  (the covariance associated with any sing landmark location estimate is determined only by the initial covariance in the vehicle location estimate.)



### EKF SLAM Complexity

- landmark 수에 따라 크게 의존한다. $O(N^3)$
  - inversion operation
- __cost per step : $O(N^2)$ -------------------?__
- Memory consumpiton : $O(N^2)$



### EKF SUMMARY

- linear gaussian 의 경우에는 converge한다.
- non-linearity가 크면 diverge할 수 있다.
- single mode만 다룬다.
  - 하나의 평균을 갖는 gaussian으로는 로봇의 위치가 한 곳이 아닌 여러 곳으로 표현할 수가 없다.
- sub-mapping 방식으로 복잡도를 감소시키는 방법이 존재한다.
- landmark의 수가 너무 많지 않은 환경에서만 사용하자.